{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin: 0 auto 30px; height: 60px; border: 2px solid gray; border-radius: 6px;\">\n",
    "  <div style=\"float: left;\"><img src=\"img/epfl.png\" /></div>\n",
    "  <div style=\"float: right; margin: 20px 30px 0; font-size: 10pt; font-weight: bold;\"><a href=\"https://moodle.epfl.ch/course/view.php?id=18253\">COM202 - Signal Processing</a></div>\n",
    "</div>\n",
    "<div style=\"clear: both; font-size: 30pt; font-weight: bold; color: #483D8B;\">\n",
    "    Lab 9: Audio signal processing\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this two-part notebook we will learn how to use discrete-time filters to extract specific audio frequencies from an audio file and we will consider some practical issues when implementing filtering algorithms in software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import IPython\n",
    "import scipy.signal as sp\n",
    "from scipy.io import wavfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (14,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with audio using SciPy\n",
    "\n",
    "The [SciPy library](https://scipy.org/) is a Python library implementing _\"fundamental algorithms for scientific computing\"_; among the various subpackages included in SciPy, the [signal processing module](https://docs.scipy.org/doc/scipy/reference/signal.html#module-scipy.signal) provides us with a rich set of ready-made routines for reading, writing, playing and filtering audio files. \n",
    "\n",
    "In the following subsections please find a brief overview of the main `scipy.signal` tools that we will use later; click on the function name to access the official online documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete-time filters\n",
    "\n",
    "In this section we will consider generic discrete-time filters with transfer function\n",
    "\n",
    "$$\n",
    "    H(z) = \\frac{b_0 + b_1 z^{-1} + \\ldots + b_{N-1}z^{-N}}{1 + a_1 z^{-1} + \\ldots + a_{N-1}z^{-M}};\n",
    "$$\n",
    "\n",
    "most SciPy functions will return or accept as input the transfer function coefficients as two distinct NumPy arrays, which by convention we will call `b` and `a`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter design\n",
    "\n",
    "Scipy provides all the fundamental filter design routines; in this notebook we will only use a few of them:\n",
    " * [`scipy.signal.butter()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.butter.html#scipy.signal.butter) to design Butterworth filters (monotonic frequency response)\n",
    " * [`scipy.signal.ellip()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.ellip.html#scipy.signal.ellip) for elliptical filters (sharp transition bands)\n",
    " * [`scipy.signal.remez()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.remez.html#scipy.signal.remez) to design linear-phase optimal FIR filters using the Parks-McClellan algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frequency response, poles, zeros\n",
    "\n",
    "Once you have the $a_k$ and $b_k$ coefficients for the transfer function, you can look at the filter properties using \n",
    "\n",
    " * [`scipy.signal.freqz()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.freqz.html#scipy.signal.freqz)  to plot the frequency response of a filter \n",
    " * [`scipy.signal.tf2zpk()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.tf2zpk.html#scipy.signal.tf2zpk) to compute the poles and the zeros of the transfer function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, the following code block designs a 6th-order lowpass elliptic filter with cutoff frequency $\\omega_c = \\pi/4,$  10% maximum ripple in passband and 40dB attenuation in stopband. The filter's coefficients are used to plot the magnitude response over the positive frequencies and show the locations of the poles and the zeros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, a = sp.ellip(6, .1, 40, 0.25, 'lowpass')\n",
    "\n",
    "w, H = sp.freqz(b, a);\n",
    "plt.plot(w, (np.abs(H)));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot unit circle\n",
    "uc = np.exp(1j * np.linspace(0, 2 * np.pi, 100))\n",
    "plt.plot(uc.real, uc.imag, 'k')\n",
    "plt.axis('equal');\n",
    "\n",
    "# plot poles and zeros\n",
    "z, p, _ = sp.tf2zpk(b, a)\n",
    "plt.plot(z.real, z.imag, 'og')\n",
    "plt.plot(p.real, p.imag, 'xr');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering data\n",
    "\n",
    "Finally, to process a data array with a given filter, you can use [`scipy.signal.lfilter()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.lfilter.html#scipy.signal.lfilter). For example, here are the first 100 samples of the impulse response of a leaky integrator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 0.95\n",
    "plt.stem(sp.lfilter([1-lam], [1, -lam], np.r_[1, np.zeros(99)]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "Plot the first 100 samples of the impulse response of the elliptic filter designed in the previous subsection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "plt.stem(sp.lfilter(b, a, np.r_[1, np.zeros(99)]));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling audio files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading and writing WAV files\n",
    "\n",
    "SciPy's [input-output module](https://docs.scipy.org/doc/scipy/reference/io.html) provides us with convenient functions to read and write audio files in the uncompressed [WAV format](https://en.wikipedia.org/wiki/WAV):\n",
    "\n",
    " * [`scipy.io.wavfile.read()`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.io.wavfile.read.html#scipy.io.wavfile.read) to load an audio file\n",
    " * [`scipy.io.wavfile.write()`](https://docs.scipy.org/doc/scipy/reference/io.html#module-scipy.io.wavfile) to write audio data to disk in WAV format\n",
    " \n",
    "`wavfile.read()` returns the audio data as a NumPy array whose elements can be in a variety of numerical formats, although most often they will be 16-bit signed integers. For convenience, since we're not concerned with memory constraints in this notebook, we will convert audio data to floating point values between -1 and 1. \n",
    "\n",
    "Note also that if the audio file is stereo, the data returned by `wavfile.read()` will be a two-dimensional array. Just to keep things simple, we will convert stereo data to mono by taking the average of the two channels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Playing audio\n",
    "\n",
    "Finally, the [IPython package](https://ipython.org/), written by the same authors of the Jupyter framework, is a Python framework geared at interactive and multimedia-based computing. In this notebook we are interested playing audio files and to this aim we will use the function `IPython.display.Audio()` which returns an interactive widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Fs, s = wavfile.read('data/sm.wav')\n",
    "IPython.display.Audio(s, rate=Fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together in a class\n",
    "\n",
    "In the rest of this notebook it will be useful to collect many audio manipulation routines in a Python class. Let's start with a simple implementation that handles reading a file and converting the data to our preferred format (mono, floating point):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioData:\n",
    "    def __init__(self, filename: str):\n",
    "        self.rate, self.data = wavfile.read(filename)\n",
    "        channels = self.data.shape[1] if len(self.data.shape) > 1 else 1\n",
    "        duration = int(self.data.size / channels / self.rate)\n",
    "        print(f'duration {duration} seconds, sampling rate {self.rate} Hz')\n",
    "        # if necessary, convert data to floating point and normalize \n",
    "        if self.data.dtype == np.int16:\n",
    "            self.data = self.data / 32768.0\n",
    "        elif self.data.dtype != np.float32:\n",
    "            raise 'unsupported WAV format'\n",
    "        # convert to mono if multichannel\n",
    "        if len(self.data.shape) > 1:\n",
    "            print('converting stereo data to mono')\n",
    "            self.data = np.sum(self.data, axis=1) / self.data.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dynamically adding methods\n",
    "\n",
    "Since we will be adding functionalities to our base class, instead of rewriting the entire class definition every time we will be adding new methods _dynamically_. Python allow us to do so via the `setattr` function.\n",
    "\n",
    "For instance, here is how we can add a `play` method to play the audio data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(self) -> object:\n",
    "    return IPython.display.Audio(self.data, rate=self.rate)\n",
    "\n",
    "setattr(AudioData, 'play', play)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the class like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = AudioData('data/sm.wav')\n",
    "audio.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: add a method to filter and play\n",
    "\n",
    "Add a `play_filtered` method to the `AudioData` class that takes the `b` and `a` coefficients of a transfer function and returns a widget that plays the filtered audio data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_filtered(self, b: np.ndarray, a: np.ndarray) -> object:\n",
    "    ... # your code here\n",
    "\n",
    "setattr(AudioData, 'play_filtered', play_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "def play_filtered(self, b: np.ndarray, a: np.ndarray) -> object:\n",
    "    return IPython.display.Audio(sp.lfilter(b, a, self.data), rate=self.rate)\n",
    "\n",
    "setattr(AudioData, 'play_filtered', play_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what happens if we filter the data with a leaky integrator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lam = 0.99\n",
    "audio.play_filtered([1-lam], [1, -lam])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Looking at the audio data\n",
    "\n",
    "When faced with a new dataset, the first thing we want to do is look at the data itself. By now we know we will want plots both in the time and in the frequency domain. Since we are dealing with audio data, it's probably convenient to introduce a small utility function that we can use to extract a portion of the audio given a start and stop time in seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_slice(self, start:float, stop:float) -> [np.ndarray, np.ndarray]:\n",
    "    start = 0 if start is None else int(start * self.rate)\n",
    "    stop = len(self.data) if stop is None else int(stop * self.rate)\n",
    "    return np.arange(start, stop) / self.rate, self.data[start:stop]\n",
    "\n",
    "setattr(AudioData, 'audio_slice', audio_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time domain\n",
    "\n",
    "Let's add a method to our AudioData class to plot a user-defined portion of the time-domain waveform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(self, start=None, stop=None):\n",
    "    plt.plot(*self.audio_slice(start, stop))\n",
    "    plt.xlabel(\"time [s]\")\n",
    "    \n",
    "setattr(AudioData, 'plot', plot)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focusing on a smaller time range shows us (if present) the driving rhythmic pulse of a piece: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio.plot(11.5, 12.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency domain\n",
    "\n",
    "The next step is being able to plot the DFT of any given portion of the signal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: plotting the magnitude spectrum\n",
    "\n",
    "Use your previous work in Lab 4 to write a function that plots the magnitude spectrum of a portion of the signal; plot only the positive frequencies and label the frequency axis in hertz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_mag_spec(self, start=None, stop=None):\n",
    "    ... # your code here\n",
    "    \n",
    "setattr(AudioData, 'plot_mag_spec', plot_mag_spec)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "def plot_mag_spec(self, start=None, stop=None):\n",
    "    mag_spec = np.abs(np.fft.fft(self.audio_slice(start, stop)[1]))\n",
    "    N = len(mag_spec) // 2\n",
    "    plt.plot(\n",
    "        np.fft.fftfreq(len(mag_spec), d=1/self.rate)[:N],\n",
    "        mag_spec[:N],\n",
    "    )\n",
    "    plt.xlabel(\"f [Hz]\")\n",
    "    \n",
    "setattr(AudioData, 'plot_mag_spec', plot_mag_spec)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio.plot_mag_spec(10,12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-frequency representation\n",
    "\n",
    "In music, as we mentioned, the rhythmic part is apparent in the time domain as very localized energy bursts, usually associated to drum hits. The harmonic content is best viewed in the frequency domain, obviously. So sometimes you want both at the same time.\n",
    "\n",
    "Fortunately, SciPy provides us with a ready-made spectrogram routine, which we can use with just a few tweaks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_spectrogram(self, start=None, stop=None):\n",
    "    f, t, spg = sp.spectrogram(self.audio_slice(start, stop)[1], self.rate)\n",
    "    spg[spg == 0] = np.min(spg[spg > 0])\n",
    "    plt.pcolormesh(t, f, 10 * np.log10(spg))\n",
    "    plt.ylabel('frequency [Hz]');\n",
    "    plt.xlabel('time [s]');\n",
    "    \n",
    "setattr(AudioData, 'plot_spectrogram', plot_spectrogram)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio.plot_spectrogram()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal level\n",
    "\n",
    "In Lab 7 we have seen how to implement a VU-meter using a Leaky integrator. Let's add this functionality to our AudioData class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: plotting the signal level\n",
    "\n",
    "Add a function `vu_meter` to the AudioData class that returns the estimated [RMS level](https://www.physik.uzh.ch/local/teaching/SPI301/LV-2015-Help/lvanlsconcepts.chm/What_Is_RMS_Level.html#:~:text=The%20RMS%20level%20of%20a,as%20noise%20or%20periodic%20signals.) of the signal over a given time span. Use a moving average filter (like in lab 7) to compute the local power. Also return the time indexes to facilitate plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vu_meter(self, start=None, stop=None, span_ms=100):\n",
    "    t, x = self.audio_slice(start, stop)\n",
    "    M = ...  # compute the length of the MA filter from span_ms\n",
    "    rms = ...  # your code here\n",
    "    return t, rms\n",
    "    \n",
    "setattr(AudioData, 'vu_meter', vu_meter)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "def vu_meter(self, start=None, stop=None, span_ms=100) -> tuple[np.ndarray, np.ndarray]:\n",
    "    t, x = self.audio_slice(start, stop)\n",
    "    M = int(span_ms * self.rate / 1000)\n",
    "    rms = np.sqrt(sp.lfilter(np.ones(M) / M, [1], x ** 2))\n",
    "    return t, rms\n",
    "    \n",
    "setattr(AudioData, 'vu_meter', vu_meter)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio.plot(0, 4)\n",
    "plt.plot(*audio.vu_meter(0, 4));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio filters\n",
    "\n",
    "In this section we will use different filters to extract different frequency regions of an audio file. Let's load one of the files available with this notebook (but you can of course use your own audio file!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio = AudioData('data/sm.wav')\n",
    "#audio = AudioData('data/groove.wav')\n",
    "\n",
    "audio.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the low frequencies\n",
    "\n",
    "Suppose we are interested in hearing more clearly the bass part; since a bass guitar plays notes with pitch from about 40 to 400 Hz, we could try to filter the audio with a lowpass with cutoff 200Hz to focus on the range of the instrument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IIR filtering\n",
    "\n",
    "Let's start with an elliptic filter for which we can use the `sp.ellip` design routine. Let's first convert the desired cutoff frequency in Hertz to a normalized value that the function understands; according to [the specs](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.ellip.html#scipy.signal.ellip), the cutoff frequency should be normalized so that a value of 1 corresponds to half the sampling frequency of the signal. \n",
    "\n",
    "We are using a 8-th order filter with 60dB attenuation in stopband and 10% max ripple. Feel free to play with these design parameters and see the effects that they have on the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = 200  # cutoff frequency in Hz\n",
    "wc = fc / (audio.rate / 2)\n",
    "b_low, a_low = sp.ellip(8, .1, 60, wc)\n",
    "\n",
    "audio.play_filtered(b_low, a_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FIR filtering\n",
    "\n",
    "We can try to extract the bass using a linear-phase FIR but, in order to match the performance of the elliptic IIR, the filter will be very long; indeed, since we are selecting a very small passband and a very high stopband attenuation, we will need to use on the order of 1000 taps.\n",
    "\n",
    "To design the filter we can use the [built-in frequency normalization](https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.remez.html#scipy.signal.remez) provided by SciPy's `remez()` function; we choose an odd number of taps in order to obtain a type I filter (symmetric with integer delay)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = 200  # cutoff frequency in Hz\n",
    "tb = 50  # width of transition band in Hz\n",
    "M = 1001  # length of the filter\n",
    "h_low = sp.remez(\n",
    "    M, \n",
    "    [0, fc-tb/2, fc+tb/2, audio.rate/2], \n",
    "    [1, 0], \n",
    "    [1, 1], \n",
    "    fs=audio.rate, \n",
    "    maxiter=50,\n",
    ")\n",
    "\n",
    "audio.play_filtered(h_low, [1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IIR vs FIR\n",
    "\n",
    "Let's compare the performance of the two filters by looking at their magnitude response. We will focus on the low-frequency part of the spectrum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "PTS = 2048  # by default freqz uses 512 points, we want more\n",
    "w, H1 = sp.freqz(b_low, a_low, PTS);\n",
    "w, H2 = sp.freqz(h_low, [1], PTS);\n",
    "\n",
    "f = np.linspace(0, audio.rate / 2, PTS)  # axis label in Hz\n",
    "\n",
    "f_max = 800  # let's look at the spectrum up to 800 Hz\n",
    "ix = int(PTS * f_max / (audio.rate / 2))\n",
    "plt.plot(f[:ix], 20 * np.log10(np.abs(H1[:ix])), label='elliptic IIR');\n",
    "plt.plot(f[:ix], 20 * np.log10(np.abs(H2[:ix])), label='optimal FIR');\n",
    "plt.xlabel(\"f [Hz]\")\n",
    "plt.ylabel('dB');\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "An FIR filter matching the 8th-order elliptic lowpass turns out to be very expensive computationally, with over one thousand operations per output sample. Such a long impulse response also introduces a processing delay of $(M-1)/2$ samples, which could be problematic in real-time applications. So, is an FIR worth the effort? \n",
    "\n",
    "To see why it might be so, consider the problem of _removing_ the bass from the original signal. The intuitive approach is to simply subtract the bass part extracted by the filter. Let's try this using the IIR:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(audio.data - sp.lfilter(b_low, a_low, audio.data), rate=audio.rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reason why this doesn't work is because the processing delay introduced by the filter causes the input and the output to be out of alignment.\n",
    "\n",
    "With a linear-phase FIR, on the other hand, we know the exact value of the processing delay, $(M-1)/2$ samples, and so we can re-align input and output by delaying the input by $(M-1)/2$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delay = int((M-1) / 2)\n",
    "diff = np.r_[np.zeros(delay), audio.data[:-delay]] - sp.lfilter(h_low, [1], audio.data)\n",
    "\n",
    "IPython.display.Audio(diff, rate=audio.rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A big win for linear phase!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the treble\n",
    "\n",
    "Just as we tried to extract the bass, we can try to extract parts of the drum pattern. Usually, we can get a good feel for the hi-hat and cymbals by keeping the frequencies above 7KHz and discarding the rest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: extracting the treble with an FIR\n",
    "\n",
    "Design an FIR highpass with cutoff 4 kHz; once again, choose an odd number of taps in order to obtain a type I filter (symmetric with integer delay). Experiment with the length of the filter until you obtain an attenuation of at least 50 dB in the stopband."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = ...  # cutoff frequency in Hz\n",
    "tb = ...  # width of transition band in Hz\n",
    "M = ...   # length of the filter\n",
    "h_high = sp.remez(...) \n",
    "\n",
    "# plot the magnitude response in dB to check attenuation in stopband\n",
    "w, H3 = sp.freqz(h_high, [1], PTS);\n",
    "plt.plot(...);\n",
    "plt.ylabel('dB');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "fc = 4000  # cutoff frequency in Hz\n",
    "tb = 50  # width of transition band in Hz\n",
    "M = 1101  # length of the filter\n",
    "h_high = sp.remez(\n",
    "    M, \n",
    "    [0, fc-tb/2, fc+tb/2, audio.rate/2], \n",
    "    [0, 1], \n",
    "    [1, 1], \n",
    "    fs=audio.rate, \n",
    "    maxiter=50,\n",
    ")\n",
    "\n",
    "w, H3 = sp.freqz(h_high, [1], PTS);\n",
    "plt.plot(f, 20 * np.log10(np.abs(H3)));\n",
    "plt.xlabel(\"f [Hz]\")\n",
    "plt.ylabel('dB');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio.play_filtered(h_high, [1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now do it all again!\n",
    "\n",
    "Go back to the beginning of this section and run all the cells after selecting a different audio file. Feel free to use your own (but convert it to WAV format first, if it's in a compressed format such as MP3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
