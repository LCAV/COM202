{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"margin: 0 auto 30px; height: 60px; border: 2px solid gray; border-radius: 6px;\">\n",
    "  <div style=\"float: left;\"><img src=\"img/epfl.png\" /></div>\n",
    "  <div style=\"float: right; margin: 20px 30px 0; font-size: 10pt; font-weight: bold;\"><a href=\"https://moodle.epfl.ch/course/view.php?id=18253\">COM202 - Signal Processing</a></div>\n",
    "</div>\n",
    "<div style=\"clear: both; font-size: 30pt; font-weight: bold; color: #483D8B;\">\n",
    "    Lab 5: Dual-Tone Multi-Frequency (DTMF) dialing\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we will build a DTMF encoder and implement a decoder based on the Fourier transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# first our usual bookkeeping\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import IPython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (14,4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "<div style=\"float: right; margin: 10px;\"><img src=\"img/phone.jpg\" width=\"250\"></div>\n",
    "\n",
    "When you use the keypad of an analog phone such as the one on the right, the sequence of dialed digits is transmitted to the equipment in the phone company's switches in the form of _dial tones_.  This was in the day before all-digital networks and cell phones were the norm, but the method is still used today for in-call option selection (\"press 4 to talk to customer service\"...).\n",
    "\n",
    "\n",
    "DTMF signaling is the way analog phones send the number dialed by a user over to the central phone office. \n",
    "\n",
    "The encoding mechanism is rather clever: the phone's keypad is arranged in a $4\\times 3$ grid and each button is associated with *two* frequencies according to this table:\n",
    "\n",
    "\n",
    "|            | **1209 Hz** | **1336 Hz** | **1477 Hz** |\n",
    "|------------|:-----------:|:-----------:|:-----------:|\n",
    "| **697 Hz** |      1      |      2      |      3      |\n",
    "| **770 Hz** |      4      |      5      |      6      |\n",
    "| **852 Hz** |      7      |      8      |      9      |\n",
    "| **941 Hz** |      *      |      0      |      #      |\n",
    "\n",
    "\n",
    "The frequencies in the table have been chosen so that they are \"coprime\"; in other words, no frequency is a rational multiple of any other, which reduces the probability of erroneously detecting the received signals due to interference. When a button is pressed, the two corresponding frequencies are generated simultaneously and sent over the line. For instance, if the digit '1' is pressed, the generated signal will be:\n",
    "\n",
    "$$\n",
    "    x(t) = \\sin(2\\pi\\cdot 1209\\cdot t) + \\sin(2\\pi\\cdot697\\cdot t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete-time implementation\n",
    "\n",
    "DTMF was developed in the late 1950s and the first commercial DTMF phones hit the market in the 1960s. At the time, the system was implemented using analog hardware and the various frequencies were generated by a set of individual electronic oscillators.\n",
    "\n",
    "Obviously in this notebook we can only use discrete-time signals, so we need to establish an invertible mapping between the real-world frequencies of the DTMF standard and their digital counterparts. This is relatively straightforward even without any formal knowledge of sampling and interpolation since all the signals involved are pure sinusoids."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digital to analog \n",
    "\n",
    "As we have seen in class, a digital-to-analog converter like the soundcard in your PC can \"play\" a discrete-time signal by outputting the samples at a constant rate $F_s$, measured in samples per second; this rate is the \"clock\" of the soundcard and it is referred to as the _sampling rate_ or sampling _frequency_ of the system. \n",
    "\n",
    "When a soundcard with sampling rate $F_s$ plays a pure discrete-time sinusoid with angular frequency $\\omega_0$, that is, a sequence of the form $x[n] = \\cos(\\omega_0 n)$, the output is a continuous-time sinusoid of the form $\\cos(2\\pi f_0 t)$ with frequency\n",
    "\n",
    "$$\n",
    "    f_0 = \\frac{\\omega_0}{2\\pi}F_s.\n",
    "$$\n",
    "\n",
    "What this means is that the pitch you hear when a discrete-time sinusoid is played by a soundcard depends _both_ on the frequency of the discrete-time sinusoid _and_ on the sampling rate of the soundcard, and the latter is usually a user-definable parameter. Here is an example: listen to how the pitch changes when the _same_ discrete-time sinusoid is played by the soundcard at different sampling rates: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "w0 = 2 * np.pi * 0.05 \n",
    "x = np.sin(w0 * np.arange(0, 8000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# let's play the sequence using 8000 samples per second\n",
    "IPython.display.Audio(x, rate=8000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now let's play the same sequence but now using 16000 samples per second\n",
    "IPython.display.Audio(x, rate=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# and now using 4000 samples per second\n",
    "IPython.display.Audio(x, rate=4000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analog to digital\n",
    "\n",
    "When used to record audio, a PC soundcard works as an analog to digital converter and _measures_ its analog input signal at a fixed rate $F_s$ to produce $F_s$ discrete-time samples every second. \n",
    "\n",
    "Again, if the input is a simple sinusoid of the form $\\sin(2\\pi f_0 t)$ the resulting discrete-time sequence will be \n",
    "\n",
    "$$\n",
    "    x[n] = \\sin(\\omega_0 n) \\qquad \\text{with} \\qquad \\omega_0 = 2\\pi\\frac{f_0}{F_s}.\n",
    "$$\n",
    "\n",
    "The only requirement here is that the sampling frequency $F_s$ must be larger than _twice_ the frequency of the input sinusoid, i.e. $F_s > 2f_0$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: aliasing\n",
    "\n",
    "\n",
    "What happens if the input to the soundcard is a sinusoid with frequency $f_0 = 1.6F_s$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "The resulting digital frequency will be \n",
    "\n",
    "$$\n",
    "    \\omega_0 = 2\\pi\\frac{f_0}{F_s} = 3.2\\pi;\n",
    "$$\n",
    "\n",
    "however, $x[n] = \\sin(3.2\\pi n) = \\sin(1.2\\pi n + 2\\pi n) = \\sin(1.2\\pi n)$, that is, the frequency has been _aliased_ to a lower frequency in discrete time. If now we play back the sequence $x[n]$ through the soundcard at the same rate we sampled it, the frequency of the resulting continuous-time sinusoid will be\n",
    "\n",
    "$$\n",
    "    f'_0 = \\frac{1.2\\pi}{2\\pi}F_s = 0.6F_s,\n",
    "$$\n",
    "\n",
    "which completely different from the original frequency!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: minimum sampling frequency\n",
    "\n",
    "What is the minimum sampling frequency $F_s$ if we want to implement a digital DTMF decoder?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "**Answer:** Since the maximum frequency in the DTMF table is 1477 Hz, we need $F_s > 2954$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "In the rest of the notebook, we will use $F_s = 8000$ since not all soundcards today offer a choice of lower sampling rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "Fs = 8000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The encoder\n",
    "\n",
    "Before designing a decoder, let's implement a DTMF encoder according to the official specifications. We already know the frequencies associated to each key; the requirements on timings are:\n",
    "\n",
    " * each tone should be at least 65ms long\n",
    " * tones corresponding to successive digits should be separated by a silent gap of at least 65ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: a DTMF encoder\n",
    "\n",
    "Complete the function below so that it returns the DTMF encoding of a series of keys passed as a string. For your convenience, the dictionary containing the DTMF frequencies is already provided, as well as the durations (in seconds) of the tones and the silence gap. (We choose the latter larger than the lower bound  65ms to distinguish them more easily.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def DTMF_encode(digits: str, Fs=8000) -> np.ndarray: \n",
    "    TONE_SEC, SPACE_SEC = 0.2, 0.1\n",
    "    DTMF_FREQS = {\n",
    "        '1': (697, 1209), '2': (697, 1336), '3': (697, 1477),\n",
    "        '4': (770, 1209), '5': (770, 1336), '6': (770, 1477),\n",
    "        '7': (852, 1209), '8': (852, 1336), '9': (852, 1477),\n",
    "        '*': (941, 1209), '0': (941, 1336), '#': (941, 1477),        \n",
    "    }\n",
    "    ...\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "def DTMF_encode(digits: str, Fs=8000) -> np.ndarray: \n",
    "    TONE_SEC, SPACE_SEC = 0.2, 0.1\n",
    "    DTMF_FREQS = {\n",
    "        '1': (697, 1209), '2': (697, 1336), '3': (697, 1477),\n",
    "        '4': (770, 1209), '5': (770, 1336), '6': (770, 1477),\n",
    "        '7': (852, 1209), '8': (852, 1336), '9': (852, 1477),\n",
    "        '*': (941, 1209), '0': (941, 1336), '#': (941, 1477),        \n",
    "    }\n",
    "\n",
    "    n = np.arange(0, int(TONE_SEC * Fs))\n",
    "    x = np.array([])\n",
    "    \n",
    "    for k in digits:\n",
    "        try:\n",
    "            w = 2 * np.pi * np.array(DTMF_FREQS[k]) / Fs \n",
    "        except KeyError:\n",
    "            print(f'invalid key: {k}')\n",
    "            return\n",
    "        x = np.r_[ x, np.sin(w[0] * n) + np.sin(w[1] * n), np.zeros(int(SPACE_SEC * Fs)) ]\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test it and evaluate it \"by ear\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = DTMF_encode('123##45', Fs=Fs)\n",
    "IPython.display.Audio(x, rate=Fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The decoder\n",
    "\n",
    "Now let's start thinking about the decoder. We will use the following strategy:\n",
    "\n",
    " * we will split the signal into individual key presses by looking at the position of the silent gaps\n",
    " * we will compute the DFT of each segment\n",
    " * we will search for the peaks in the DFT magnitude to recover the frequencies in each tone"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Signal segmentation\n",
    "\n",
    "In order to split the signal, the idea is to find the points where the signal goes from silence to a tone. Since silence corresponds to samples with an amplitude of zero, the idea is to detect the parts of the signal where the energy is below a certain threshold. We will use a short **analysis window** to process the signal one chunk at a time, compute the power (i.e. the average energy) of the signal for each chunk, and use the resulting values to mark each chunk as belonging to a silent gap or to a tone.\n",
    "\n",
    "Let's see how we can do that; let's look at the raw data first:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(x);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems clear that the high and low energy regions of the signal should be relatively easy to find. However, we don't really know yet what value to use for the threshold. Let's first see what happens if we compute the power of the signal over 10ms chunks. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: local power values\n",
    "\n",
    "In the cell below, complete the code for a function that computes the _average_ energy (that is, the power) of a signal over successive chunks of fixed length. To do so:\n",
    " - reshape the signal into a two-dimensional array so that each row contains a number of samples corresponding to the size of the analysis window \n",
    " - compute the each chunk's power as the average of the squares of the samples\n",
    " - return two arrays:\n",
    "   - the first array should contain the sample number corresponding to the beginning of each chunk\n",
    "   - the second array should contain the power of the corresponding chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def local_power(x: np.ndarray, chunk_len_ms: int, Fs=8000) -> tuple[np.ndarray, np.ndarray]:\n",
    "    # number of samples per analysis window\n",
    "    chunk_len = ...\n",
    "    # truncate the signal to a length multiple of `chunk_len` to avoid reshaping errors\n",
    "    w = ...\n",
    "    # reshape the signal \n",
    "    w = ...\n",
    "    # compute the power for each chunk \n",
    "    power_per_chunk = ...\n",
    "    return chunk_len * np.arange(0, w.shape[0]), power_per_chunk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "def local_power(x: np.ndarray, chunk_len_ms: int, Fs=8000) -> tuple[np.ndarray, np.ndarray]:\n",
    "    chunk_len = int(chunk_len_ms * Fs / 1000)\n",
    "    w = x[:chunk_len * (len(x) // chunk_len)]\n",
    "    w = np.reshape(w, (-1, chunk_len))\n",
    "    power_per_chunk = np.sum(w * w, axis=1) / chunk_len\n",
    "    return chunk_len * np.arange(0, w.shape[0]), power_per_chunk "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(*local_power(x, 10));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the plot it looks like a value of 0.5 for the threshold should do the trick. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting the signal\n",
    "\n",
    "The following function will return an array containing the start and stop indexes of each individual tone in the DTMF signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def DTMF_split(x: np.ndarray, threshold=0.5, chunk_len_ms=10, Fs=8000) -> list[tuple[int, int]]:\n",
    "    ix, power = local_power(x, chunk_len_ms, Fs) # compute local energy over contiguous chunks\n",
    "    tone_boundaries = [] \n",
    "    tone_start = 0\n",
    "    # simple state machine to iterate through chunks, aternating between \"silence\" and \"tone\"\n",
    "    state = 'silence'\n",
    "    for n in range(0, len(power)):\n",
    "        if state == 'silence':\n",
    "            if power[n] > threshold:\n",
    "                tone_start = ix[n] # transition from silence to tone: tone starting\n",
    "                state = 'tone'\n",
    "        elif state == 'tone':\n",
    "            if power[n] < threshold: # energy dropped below threshold, tone ends and silence starts\n",
    "                tone_boundaries.append( (tone_start, ix[n]) )\n",
    "                state = 'silence'\n",
    "    # this takes care of the last tone, if not followed by silence\n",
    "    if state == 'tone':    \n",
    "        tone_boundaries.append( (tone_start, len(x)) )\n",
    "    return tone_boundaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's check it out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "plt.plot(x);\n",
    "for tone in chain.from_iterable(DTMF_split(x)):\n",
    "    plt.axvline(tone, color='red')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequency identification\n",
    "\n",
    "<div style=\"text-align:center\">\n",
    "<img src=\"img/MatrixGIFFT.gif\" width=500 height=500>\n",
    "</div>\n",
    "\n",
    "Now that we have a splitter, let's run a DFT over the tone sections and find the DTMF frequencies that are closest to the peaks of the DFT magnitude. \n",
    "\n",
    "The \"low\" DTMF frequencies are in the 697 Hz to 941 Hz range, while the high frequencies in the 1209 Hz to 1477 Hz range, so we will look for a DFT peak in each of those intervals. For instance, let's look at the first tone, and let's look at the peaks in the DFT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = abs(np.fft.fft(x[0:2400]))\n",
    "plt.plot(X[0:500]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We clearly have identifiable peaks. The only thing we need to pay attention to is how to go from the DFT indexes to real-world frequencies and back. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: mapping hertz to DFT index (and vice-versa)\n",
    "\n",
    "Complete the 2 functions below so that they perform the following tasks:\n",
    " - `Hz_to_DFT(frequencies, N, Fs)` takes a list of frequency values in hertz and returns the corresponding DFT indexes for a DFT of length `N` and sampling frequency `Fs` \n",
    "- `DFT_to_Hz(indexes, N, Fs)` takes a list of DFT indexes for a DFT of length `N` and returns the corresponding frequencies in hertz for a sampling frequency `Fs` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def Hz_to_DFT(frequencies, N, Fs=8000):\n",
    "    ...\n",
    "\n",
    "def DFT_to_Hz(indexes, N, Fs=8000):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "def Hz_to_DFT(frequencies, N, Fs=8000):\n",
    "    return (N * np.array(frequencies) / Fs).astype(int)\n",
    "\n",
    "def DFT_to_Hz(indexes, N, Fs=8000):\n",
    "    return np.array(indexes).astype(float) / N * Fs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test your work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "f, k, N = (32, 24, 80), (4, 3, 10), 1000 \n",
    "if np.sum(\n",
    "    (Hz_to_DFT(f, N, Fs) - k) +   # Hz -> DFT correct\n",
    "    (DFT_to_Hz(k, N, Fs) - f) +   # DFT -> Hz correct\n",
    "    (Hz_to_DFT(DFT_to_Hz(k, N, Fs), N, Fs) - k)  # DFT -> Hz -> DFT correct\n",
    "  ) == 0:\n",
    "    print('good job')\n",
    "else:\n",
    "    print(t, 'sorry, try again...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: find frequency peaks\n",
    "\n",
    "Complete the function below so that it returns the frequency (in Hz) corresponding to the location of the largest spectral peak in the DFT of the data chunk for each of the ranges provided as an input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_frequency_peaks(\n",
    "        data_chunk: np.ndarray, \n",
    "        ranges: list[tuple[float, float]], \n",
    "        Fs=8000,\n",
    ") -> list[float]:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# SOLUTION:\n",
    "\n",
    "def find_frequency_peaks(\n",
    "        data_chunk: np.ndarray, \n",
    "        ranges: list[tuple[float, float]], \n",
    "        Fs=8000,\n",
    ") -> list[float]:\n",
    "    X = np.abs(np.fft.fft(data_chunk))\n",
    "    peaks = []\n",
    "    for kr in [ Hz_to_DFT(r, len(X), Fs) for r in ranges ]:\n",
    "        k_max = kr[0] + np.argmax(X[slice(*kr)])\n",
    "        peaks.append(DFT_to_Hz(k_max, len(X), Fs))\n",
    "    return peaks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Applying the function to the previous data chunk should return two frequency values that are close to or equal to the DTMF frequencies associated to key \"1\", that is, 697 Hz and 1209 Hz:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "find_frequency_peaks(x[0:2400], [(697, 941), (1209, 1477)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The full decoder\n",
    "\n",
    "Here is a full decoder for a DTMF signal, using all the building blocks developed previously:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def DTMF_decode(x: np.ndarray, threshold=0.5, chunk_len_ms=10, Fs=8000) -> list[str]:\n",
    "    # rearrange DTMF data for decoding\n",
    "    LO_FREQS = np.array([697.0, 770.0, 852.0, 941.0]) # DTMF rows\n",
    "    HI_FREQS = np.array([1209.0, 1336.0, 1477.0])     # DTMF columns\n",
    "    ranges = [ [np.min(freqs), np.max(freqs)] for freqs in [LO_FREQS, HI_FREQS] ]\n",
    "    # DTMF matrix\n",
    "    KEYS = [['1', '2', '3'], \n",
    "            ['4', '5', '6'], \n",
    "            ['7', '8', '9'], \n",
    "            ['*', '0', '#']] \n",
    "    \n",
    "    number = []\n",
    "    for tone_boundaries in DTMF_split(x, threshold, chunk_len_ms, Fs):\n",
    "        f = find_frequency_peaks(x[slice(*tone_boundaries)], ranges, Fs)\n",
    "        if len(f) != 2:\n",
    "            print(\"input signal between n={tone[0]} and n={tone[1]} is not a valid DTMF tone\")\n",
    "        else:\n",
    "            row = np.argmin(abs(LO_FREQS - f[0]))\n",
    "            col = np.argmin(abs(HI_FREQS - f[1]))\n",
    "            number.append(KEYS[row][col])\n",
    "    return number            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DTMF_decode(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yay! It works! As always, in communication systems, the receiver is often much more complicated than the sender. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Practical considerations\n",
    "\n",
    "Obviously we used a very simplified setup and we have glossed over a lot of practical details. For instance, in the splitting function, the thresholds are not determined dynamically and this may create problems in the presence of noise. Similarly, we just detect a frequency peak in the spectrum, but noise may make things more complicated. \n",
    "\n",
    "For instance, listen to the following noise-corrupted version of the original signal. Although the tones are still detectable by ear, the segmentation algorithm fails and returns a single digit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "noisy = x + np.random.uniform(-2.5, 2.5, len(x))\n",
    "IPython.display.Audio(noisy, rate=Fs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DTMF_decode(noisy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: finding the decoding parameters\n",
    "\n",
    "Use the `local_power()` function defined above to plot the per-chunk power of the noisy signal using increasingly longer chunk sizes (e.g. from 10ms to 60ms in 10ms increments). Look at the graph to determine what chunk size and what threshold value to pass to `DTMF_decode()` so that the noisy signal is correctly decoded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "# SOLUTION\n",
    "\n",
    "for chunk_len_ms in [10, 30, 40, 60]:\n",
    "    plt.plot(*local_power(noisy, chunk_len_ms), label=f'{chunk_len_ms}ms chunks');\n",
    "plt.legend();\n",
    "plt.show();\n",
    "\n",
    "# from the plot, it appears that we should choose a chunk size of 40ms or more and a threshold of about 2.5\n",
    "DTMF_decode(noisy, threshold=2.5, chunk_len_ms=40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
